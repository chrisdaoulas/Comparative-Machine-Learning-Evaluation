As a data scientist, I developed this Jupyter Notebook to evaluate multiple machine learning models for a classification task, focusing on handling class imbalance and overfitting. The project installs dependencies like PyVis and CatBoost, imports necessary libraries, and trains six models: Neural Network, Logistic Regression, CatBoost, SVM, XGBoost, and Random Forest. CatBoost emerged as the top performer with a test F1 score of 0.9015, showing strong generalization, while XGBoost (0.9071) and Random Forest (0.9046) followed closely but with some overfitting concerns. The Neural Network underperformed (F1: 0.0659) due to instability, and Logistic Regression dropped on the test set (F1: 0.155). Feature importance highlighted variables 68 and 135. The notebook includes SHAP analysis for interpretability and recommendations for improvements, such as regularization, early stopping, class weighting, and hybrid sampling techniques like SMOTE-Tomek to enhance minority-class performance, demonstrating a comprehensive approach to model selection and optimization.
